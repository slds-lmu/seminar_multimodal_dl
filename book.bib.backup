@ARTICLE{Lu2020,
  AUTHOR = {Yu, Jun and Li, Jing and Yu, Zhou and Huang, Qingming},
  DATE = {2020},
  DOI = {10.1109/TCSVT.2019.2947482},
  JOURNALTITLE = {IEEE Transactions on Circuits and Systems for Video Technology},
  NUMBER = {12},
  PAGES = {4467--4480},
  TITLE = {Multimodal Transformer With Multi-View Visual Representation for Image Captioning},
  VOLUME = {30},
}

@MISC{Fedus2021,
  AUTHOR = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2101.03961},
  DATE = {2021},
  DOI = {10.48550/ARXIV.2101.03961},
  KEYWORDS = {Machine Learning (cs.LG),Artificial Intelligence (cs.AI),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
}

@MISC{Mustafa2022,
  AUTHOR = {Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2206.02770},
  DATE = {2022},
  DOI = {10.48550/ARXIV.2206.02770},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts},
}

@ARTICLE{Carion2020,
  AUTHOR = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  URL = {https://arxiv.org/abs/2005.12872},
  DATE = {2020},
  EPRINT = {2005.12872},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {End-to-End Object Detection with Transformers},
}

@MISC{Crawshaw2020,
  AUTHOR = {Crawshaw, Michael},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2009.09796},
  DATE = {2020},
  DOI = {10.48550/ARXIV.2009.09796},
  KEYWORDS = {Machine Learning (cs.LG),Computer Vision and Pattern Recognition (cs.CV),Machine Learning (stat.ML),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Multi-Task Learning with Deep Neural Networks: A Survey},
}

@ARTICLE{Baltrusaitis2019,
  AUTHOR = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  DATE = {2019},
  DOI = {10.1109/TPAMI.2018.2798607},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {2},
  PAGES = {423--443},
  TITLE = {Multimodal Machine Learning: A Survey and Taxonomy},
  VOLUME = {41},
}

@ARTICLE{Kaiser2017,
  AUTHOR = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  URL = {https://arxiv.org/pdf/1706.05137.pdf},
  DATE = {2017},
  JOURNALTITLE = {arXiv},
  TITLE = {One Model To Learn Them All},
}

@INPROCEEDINGS{Hu2021,
  AUTHOR = {Hu, Ronghang and Singh, Amanpreet},
  BOOKTITLE = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  DATE = {2021},
  DOI = {10.1109/ICCV48922.2021.00147},
  PAGES = {1419--1429},
  TITLE = {UniT: Multimodal Multitask Learning with a Unified Transformer},
}

@MISC{Li2019,
  AUTHOR = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1908.03557},
  DATE = {2019},
  DOI = {10.48550/ARXIV.1908.03557},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Computation and Language (cs.CL),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {VisualBERT: A Simple and Performant Baseline for Vision and Language},
}

@ONLINE{Dean21,
  AUTHOR = {Dean, Jeff},
  URL = {https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/},
  DATE = {2021},
  TITLE = {Introducing Pathways: A next-generation AI architecture},
  URLDATE = {2022-08-23},
}

@ARTICLE{Krishna2017,
  AUTHOR = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
  LOCATION = {USA},
  PUBLISHER = {Kluwer Academic Publishers},
  URL = {https://DOI.org/10.1007/s11263-016-0981-7},
  DATE = {2017-05},
  DOI = {10.1007/s11263-016-0981-7},
  ISSN = {0920-5691},
  JOURNALTITLE = {Int. J. Comput. Vision},
  KEYWORDS = {Language,Relationships,Attributes,Question answering,Scene graph,Crowdsourcing,Computer vision,Knowledge,Image,Objects,Dataset},
  NUMBER = {1},
  PAGES = {32--73},
  TITLE = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  VOLUME = {123},
}

@INPROCEEDINGS{Wang2022,
  ABSTRACT = {In this work, we pursue a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision &amp; language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.},
  AUTHOR = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  EDITOR = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  PUBLISHER = {PMLR},
  URL = {https://proceedings.mlr.press/v162/wang22al.html},
  BOOKTITLE = {Proceedings of the 39th International Conference on Machine Learning},
  DATE = {2022},
  FILE = {https://proceedings.mlr.press/v162/wang22al/wang22al.pdf},
  PAGES = {23318--23340},
  SERIES = {Proceedings of Machine Learning Research},
  TITLE = {{OFA}: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  VOLUME = {162},
}

@MISC{Reed2022,
  AUTHOR = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2205.06175},
  DATE = {2022},
  DOI = {10.48550/ARXIV.2205.06175},
  KEYWORDS = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Machine Learning (cs.LG),Robotics (cs.RO),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {A Generalist Agent},
}

@ARTICLE{Chowdhery2022,
  AUTHOR = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  URL = {https://arxiv.org/abs/2204.02311},
  DATE = {2022},
  JOURNALTITLE = {arxiv:2204.02311},
  TITLE = {PaLM: Scaling Language Modeling with Pathways},
}

@MISC{Yu2022,
  AUTHOR = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2206.10789},
  DATE = {2022},
  DOI = {10.48550/ARXIV.2206.10789},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
}

@INPROCEEDINGS{Fernando2017,
  AUTHOR = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
  URL = {https://arxiv.org/abs/1701.08734},
  DATE = {2017},
  TITLE = {PathNet: Evolution Channels Gradient Descent in Super Neural Networks},
}

@INPROCEEDINGS{He2016b,
  AUTHOR = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  EDITOR = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {Computer Vision -- ECCV 2016},
  DATE = {2016},
  ISBN = {978-3-319-46493-0},
  PAGES = {630--645},
  TITLE = {Identity Mappings in Deep Residual Networks},
}

@INPROCEEDINGS{Dean20,
  AUTHOR = {Dean, Jeffrey},
  BOOKTITLE = {2020 IEEE International Solid- State Circuits Conference - (ISSCC)},
  DATE = {2020},
  DOI = {10.1109/ISSCC19947.2020.9063049},
  PAGES = {8--14},
  TITLE = {1.1 The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design},
}

@REPORT{Lewkowycz2022,
  AUTHOR = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David Martin and Dyer, Ethan S and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and Wu, Yuhuai and Neyshabur, Behnam and Gur-Ari, Guy and Misra, Vedant},
  URL = {https://arxiv.org/abs/2206.14858},
  DATE = {2022},
  TITLE = {Solving Quantitative Reasoning Problems with Language Models},
  TYPE = {techreport},
}

@INPROCEEDINGS{Riquelme2021,
  AUTHOR = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, André and Keysers, Daniel and Houlsby, Neil},
  EDITOR = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2021},
  PAGES = {8583--8595},
  TITLE = {Scaling Vision with Sparse Mixture of Experts},
  VOLUME = {34},
}

@MISC{Gesmundo2022a,
  AUTHOR = {Gesmundo, Andrea and Dean, Jeff},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2205.10937},
  DATE = {2022},
  DOI = {10.48550/ARXIV.2205.10937},
  KEYWORDS = {Machine Learning (cs.LG),Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),Neural and Evolutionary Computing (cs.NE),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {muNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-tuning Multitask Systems},
}

@ARTICLE{Steiner2021,
  AUTHOR = {Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2106.10270},
  DATE = {2021},
  DOI = {10.48550/ARXIV.2106.10270},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Artificial Intelligence (cs.AI),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
}

@INPROCEEDINGS{Houlsby2019,
  ABSTRACT = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8%$ of the performance of full fine-tuning, adding only $3.6%$ parameters per task. By contrast, fine-tuning trains $100%$ of the parameters per task.},
  AUTHOR = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  EDITOR = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  PUBLISHER = {PMLR},
  URL = {https://proceedings.mlr.press/v97/houlsby19a.html},
  BOOKTITLE = {Proceedings of the 36th International Conference on Machine Learning},
  DATE = {2019},
  FILE = {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  PAGES = {2790--2799},
  SERIES = {Proceedings of Machine Learning Research},
  TITLE = {Parameter-Efficient Transfer Learning for {NLP}},
  VOLUME = {97},
}

@INPROCEEDINGS{Rebuffi2017,
  AUTHOR = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  EDITOR = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2017},
  TITLE = {Learning multiple visual domains with residual adapters},
  VOLUME = {30},
}

@MISC{Bilen2017,
  AUTHOR = {Bilen, Hakan and Rebuffi, SSylvestre and Jakab, Tomas},
  DATE = {2017},
  TITLE = {Visual domain decathlon},
}

@ARTICLE{Doerr2021,
  AUTHOR = {Doerr, Benjamin and Neumann, Frank},
  LOCATION = {New York, NY, USA},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://doi.org/10.1145/3472304},
  DATE = {2021-10},
  DOI = {10.1145/3472304},
  ISSN = {2688-299X},
  JOURNALTITLE = {ACM Trans. Evol. Learn. Optim.},
  KEYWORDS = {parameterized complexity,discrete optimization,evolutionary algorithms,estimation of distribution algorithms,Theory},
  NUMBER = {4},
  TITLE = {A Survey on Recent Progress in the Theory of Evolutionary Algorithms for Discrete Optimization},
  VOLUME = {1},
}

@ARTICLE{Baeck1993,
  AUTHOR = {Bäck, Thomas and Schwefel, Hans-Paul},
  DATE = {1993},
  DOI = {10.1162/evco.1993.1.1.1},
  JOURNALTITLE = {Evolutionary Computation},
  NUMBER = {1},
  PAGES = {1--23},
  TITLE = {An Overview of Evolutionary Algorithms for Parameter Optimization},
  VOLUME = {1},
}

@MISC{Hinton2015,
  AUTHOR = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1503.02531},
  DATE = {2015},
  DOI = {10.48550/ARXIV.1503.02531},
  KEYWORDS = {Machine Learning (stat.ML),Machine Learning (cs.LG),Neural and Evolutionary Computing (cs.NE),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Distilling the Knowledge in a Neural Network},
}

@INPROCEEDINGS{Shaazer2017,
  AUTHOR = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  URL = {https://openreview.net/pdf?id=B1ckMDqlg},
  DATE = {2017},
  TITLE = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
}

@ARTICLE{Jordan1994,
  AUTHOR = {Jordan, Michael I. and Jacobs, Robert A.},
  DATE = {1994},
  DOI = {10.1162/neco.1994.6.2.181},
  JOURNALTITLE = {Neural Computation},
  NUMBER = {2},
  PAGES = {181--214},
  TITLE = {Hierarchical Mixtures of Experts and the EM Algorithm},
  VOLUME = {6},
}

@ARTICLE{Jacobs1991,
  AUTHOR = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  DATE = {1991},
  DOI = {10.1162/neco.1991.3.1.79},
  JOURNALTITLE = {Neural Computation},
  NUMBER = {1},
  PAGES = {79--87},
  TITLE = {Adaptive Mixtures of Local Experts},
  VOLUME = {3},
}

@INPROCEEDINGS{sennrich-etal-2016-neural,
  AUTHOR = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  LOCATION = {Berlin, Germany},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/P16-1162},
  BOOKTITLE = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  DATE = {2016-08},
  DOI = {10.18653/v1/P16-1162},
  PAGES = {1715--1725},
  TITLE = {Neural Machine Translation of Rare Words with Subword Units},
}

@INPROCEEDINGS{pmlr-v139-ramesh21a,
  ABSTRACT = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  AUTHOR = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  EDITOR = {Meila, Marina and Zhang, Tong},
  PUBLISHER = {PMLR},
  URL = {https://proceedings.mlr.press/v139/ramesh21a.html},
  BOOKTITLE = {Proceedings of the 38th International Conference on Machine Learning},
  DATE = {2021},
  FILE = {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  PAGES = {8821--8831},
  SERIES = {Proceedings of Machine Learning Research},
  TITLE = {Zero-Shot Text-to-Image Generation},
  VOLUME = {139},
}

@ARTICLE{ResNet,
  AUTHOR = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  URL = {http://arxiv.org/abs/1512.03385},
  DATE = {2015},
  EPRINT = {1512.03385},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {Deep Residual Learning for Image Recognition},
}

@INPROCEEDINGS{mccoco,
  AUTHOR = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {Computer Vision -- ECCV 2014},
  DATE = {2014},
  ISBN = {978-3-319-10602-1},
  PAGES = {740--755},
  TITLE = {Microsoft COCO: Common Objects in Context},
}

@INPROCEEDINGS{kudo-richardson-2018-sentencepiece,
  ABSTRACT = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.},
  AUTHOR = {Kudo, Taku and Richardson, John},
  LOCATION = {Brussels, Belgium},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/D18-2012},
  BOOKTITLE = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  DATE = {2018-11},
  DOI = {10.18653/v1/D18-2012},
  PAGES = {66--71},
  TITLE = {{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
}

@ARTICLE{Devlin2018,
  AUTHOR = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  DATE = {2018-10-11},
  EPRINT = {1810.04805},
  EPRINTCLASS = {cs.CL},
  EPRINTTYPE = {arXiv},
  FILE = {:http\://arxiv.org/pdf/1810.04805v2:PDF},
  KEYWORDS = {cs.CL},
  TITLE = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
}

@ARTICLE{brown2020language,
  AUTHOR = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  DATE = {2020},
  JOURNALTITLE = {Advances in neural information processing systems},
  PAGES = {1877--1901},
  TITLE = {Language models are few-shot learners},
  VOLUME = {33},
}

@ARTICLE{ImageNet,
  ABSTRACT = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  AUTHOR = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  LOCATION = {USA},
  PUBLISHER = {Kluwer Academic Publishers},
  URL = {https://doi.org/10.1007/s11263-015-0816-y},
  DATE = {2015-12},
  DOI = {10.1007/s11263-015-0816-y},
  ISSN = {0920-5691},
  JOURNALTITLE = {Int. J. Comput. Vision},
  KEYWORDS = {Benchmark,Object detection,Large-scale,Object recognition,Dataset},
  NUMBER = {3},
  PAGES = {211--252},
  TITLE = {ImageNet Large Scale Visual Recognition Challenge},
  VOLUME = {115},
}

@ARTICLE{dosovitskiy2020image,
  AUTHOR = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2010.11929},
  TITLE = {An image is worth 16x16 words: Transformers for image recognition at scale},
}

@ARTICLE{vaswani2017attention,
  AUTHOR = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {Ł}ukasz and Polosukhin, Illia},
  DATE = {2017},
  JOURNALTITLE = {Advances in neural information processing systems},
  TITLE = {Attention is all you need},
  VOLUME = {30},
}

@INPROCEEDINGS{deng2009imagenet,
  AUTHOR = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  ORGANIZATION = {Ieee},
  BOOKTITLE = {2009 IEEE conference on computer vision and pattern recognition},
  DATE = {2009},
  PAGES = {248--255},
  TITLE = {Imagenet: A large-scale hierarchical image database},
}

@ARTICLE{parti,
  AUTHOR = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing and Luong, Thang and Baid, Gunjan and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  DATE = {2022-06},
  DOI = {10.48550/arXiv.2206.10789},
  TITLE = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
}

@INPROCEEDINGS{lewis-etal-2020-bart,
  ABSTRACT = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.},
  AUTHOR = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  LOCATION = {Online},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/2020.acl-main.703},
  BOOKTITLE = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  DATE = {2020-07},
  DOI = {10.18653/v1/2020.acl-main.703},
  PAGES = {7871--7880},
  TITLE = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
}

@ARTICLE{atari,
  ABSTRACT = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
  AUTHOR = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  LOCATION = {El Segundo, CA, USA},
  PUBLISHER = {AI Access Foundation},
  DATE = {2013-05},
  ISSN = {1076-9757},
  JOURNALTITLE = {J. Artif. Int. Res.},
  NUMBER = {1},
  PAGES = {253--279},
  TITLE = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
  VOLUME = {47},
}

@ONLINE{darkMatter,
  AUTHOR = {Yann, Lecun and Ishan, Misra},
  URL = {https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/},
  DATE = {2021},
  TITLE = {Self-supervised learning: The dark matter of intelligence},
  URLDATE = {2022-06-26},
}

@ONLINE{redditUsers,
  AUTHOR = {MICHAEL BARTHEL, GALEN STOCKING, JESSE HOLCOMB and MITCHELL, AMY},
  URL = {https://www.pewresearch.org/journalism/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/},
  DATE = {2016},
  TITLE = {Reddit news users more likely to be male, young and digital in their news preferences},
  URLDATE = {2022-08-07},
}

@ONLINE{coco_eval,
  AUTHOR = {Mircosoft},
  URL = {https://cocodataset.org/#detection-eval},
  DATE = {2019},
  TITLE = {Evaluate:Detection},
  URLDATE = {2022-07-09},
}

@ONLINE{unsupBrain,
  AUTHOR = {Mineault, Patrick},
  URL = {https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/},
  DATE = {2021},
  TITLE = {Unsupervised models of the brain},
  URLDATE = {2022-06-26},
}

@ARTICLE{zhuang2021unsupervised,
  AUTHOR = {Zhuang, Chengxu and Yan, Siming and Nayebi, Aran and Schrimpf, Martin and Frank, Michael C and DiCarlo, James J and Yamins, Daniel LK},
  PUBLISHER = {National Acad Sciences},
  DATE = {2021},
  JOURNALTITLE = {Proceedings of the National Academy of Sciences},
  NUMBER = {3},
  PAGES = {e2014196118},
  TITLE = {Unsupervised neural network models of the ventral visual stream},
  VOLUME = {118},
}

@ARTICLE{liu2019roberta,
  AUTHOR = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1907.11692},
  TITLE = {Roberta: A robustly optimized bert pretraining approach},
}

@ARTICLE{bromley1993signature,
  AUTHOR = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and Säckinger, Eduard and Shah, Roopak},
  DATE = {1993},
  JOURNALTITLE = {Advances in neural information processing systems},
  TITLE = {Signature verification using a" siamese" time delay neural network},
  VOLUME = {6},
}

@ARTICLE{grill2020bootstrap,
  AUTHOR = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  DATE = {2020},
  JOURNALTITLE = {Advances in neural information processing systems},
  PAGES = {21271--21284},
  TITLE = {Bootstrap your own latent-a new approach to self-supervised learning},
  VOLUME = {33},
}

@INPROCEEDINGS{caron2021emerging,
  AUTHOR = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  BOOKTITLE = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  DATE = {2021},
  PAGES = {9650--9660},
  TITLE = {Emerging properties in self-supervised vision transformers},
}

@INPROCEEDINGS{mahajan2018exploring,
  AUTHOR = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and Van Der Maaten, Laurens},
  BOOKTITLE = {Proceedings of the European conference on computer vision (ECCV)},
  DATE = {2018},
  PAGES = {181--196},
  TITLE = {Exploring the limits of weakly supervised pretraining},
}

@ARTICLE{kolesnikov2019large,
  AUTHOR = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  PUBLISHER = {arXiv},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1912.11370},
  NUMBER = {8},
  TITLE = {Large scale learning of general visual representations for transfer},
  VOLUME = {2},
}

@ARTICLE{rajpurkar2016squad,
  AUTHOR = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  DATE = {2016},
  JOURNALTITLE = {arXiv preprint arXiv:1606.05250},
  TITLE = {Squad: 100,000+ questions for machine comprehension of text},
}

@ARTICLE{rajpurkar2018know,
  AUTHOR = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  DATE = {2018},
  JOURNALTITLE = {arXiv preprint arXiv:1806.03822},
  TITLE = {Know what you don't know: Unanswerable questions for SQuAD},
}

@ARTICLE{srivastava2022beyond,
  AUTHOR = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and others},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2206.04615},
  TITLE = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
}

@ARTICLE{bowman2021will,
  AUTHOR = {Bowman, Samuel R and Dahl, George E},
  DATE = {2021},
  JOURNALTITLE = {arXiv preprint arXiv:2104.02145},
  TITLE = {What Will it Take to Fix Benchmarking in Natural Language Understanding?},
}

@ARTICLE{goodfellow2014explaining,
  AUTHOR = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  DATE = {2014},
  JOURNALTITLE = {arXiv preprint arXiv:1412.6572},
  TITLE = {Explaining and harnessing adversarial examples},
}

@INPROCEEDINGS{recht2019imagenet,
  AUTHOR = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  ORGANIZATION = {PMLR},
  BOOKTITLE = {International Conference on Machine Learning},
  DATE = {2019},
  PAGES = {5389--5400},
  TITLE = {Do imagenet classifiers generalize to imagenet?},
}

@ARTICLE{beyer2020we,
  AUTHOR = {Beyer, Lucas and Hénaff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, Aäron van den},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2006.07159},
  TITLE = {Are we done with imagenet?},
}

@ARTICLE{li2022mask,
  AUTHOR = {Li, Feng and Zhang, Hao and Liu, Shilong and Zhang, Lei and Ni, Lionel M and Shum, Heung-Yeung and others},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2206.02777},
  TITLE = {Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation},
}

@INPROCEEDINGS{koehn2005europarl,
  AUTHOR = {Koehn, Philipp},
  BOOKTITLE = {Proceedings of machine translation summit x: papers},
  DATE = {2005},
  PAGES = {79--86},
  TITLE = {Europarl: A parallel corpus for statistical machine translation},
}

@MISC{Gokaslan2019OpenWeb,
  AUTHOR = {Gokaslan, Aaron and Cohen, Vanya},
  DATE = {2019},
  TITLE = {OpenWebText Corpus},
}

@ARTICLE{xue2020mt5,
  AUTHOR = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2010.11934},
  TITLE = {mT5: A massively multilingual pre-trained text-to-text transformer},
}

@ARTICLE{wenzek2019ccnet,
  AUTHOR = {Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzmán, Francisco and Joulin, Armand and Grave, Edouard},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1911.00359},
  TITLE = {Ccnet: Extracting high quality monolingual datasets from web crawl data},
}

@ARTICLE{bandy2021addressing,
  AUTHOR = {Bandy, Jack and Vincent, Nicholas},
  DATE = {2021},
  JOURNALTITLE = {arXiv preprint arXiv:2105.05241},
  TITLE = {Addressing" documentation debt" in machine learning research: A retrospective datasheet for bookcorpus},
}

@ARTICLE{gao2017knowledge,
  AUTHOR = {Gao, Jiyang and Li, Zhen and Nevatia, Ram and others},
  DATE = {2017},
  JOURNALTITLE = {arXiv preprint arXiv:1711.07607},
  TITLE = {Knowledge concentration: Learning 100k object classifiers in a single CNN},
}

@INPROCEEDINGS{shao2019objects365,
  AUTHOR = {Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  BOOKTITLE = {Proceedings of the IEEE/CVF international conference on computer vision},
  DATE = {2019},
  PAGES = {8430--8439},
  TITLE = {Objects365: A large-scale, high-quality dataset for object detection},
}

@ARTICLE{yuan2022wudaomm,
  AUTHOR = {Yuan, Sha and Shuai, Zhao and Jiahong, Leng and Zhao, Xue and Hanyu, Zhao and Jie, Tang},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2203.11480},
  TITLE = {WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models},
}

@INPROCEEDINGS{srinivasan2021wit,
  AUTHOR = {Srinivasan, Krishna and Raman, Karthik and Chen, Jiecao and Bendersky, Michael and Najork, Marc},
  BOOKTITLE = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  DATE = {2021},
  PAGES = {2443--2449},
  TITLE = {Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning},
}

@ARTICLE{tiedemann2018emerging,
  AUTHOR = {Tiedemann, Jörg},
  DATE = {2018},
  JOURNALTITLE = {arXiv preprint arXiv:1802.00273},
  TITLE = {Emerging language spaces learned from massively multilingual corpora},
}

@ARTICLE{mayer2014creating,
  AUTHOR = {Mayer, Thomas and Cysouw, Michael},
  DATE = {2014},
  JOURNALTITLE = {Oceania},
  NUMBER = {273},
  PAGES = {40},
  TITLE = {Creating a massively parallel Bible corpus},
  VOLUME = {135},
}

@INPROCEEDINGS{zellers2019recognition,
  AUTHOR = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  BOOKTITLE = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  DATE = {2019},
  PAGES = {6720--6731},
  TITLE = {From recognition to cognition: Visual commonsense reasoning},
}

@INPROCEEDINGS{antol2015vqa,
  AUTHOR = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  BOOKTITLE = {Proceedings of the IEEE international conference on computer vision},
  DATE = {2015},
  PAGES = {2425--2433},
  TITLE = {Vqa: Visual question answering},
}

@INPROCEEDINGS{zhang2016yin,
  AUTHOR = {Zhang, Peng and Goyal, Yash and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  BOOKTITLE = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  DATE = {2016},
  PAGES = {5014--5022},
  TITLE = {Yin and yang: Balancing and answering binary visual questions},
}

@INPROCEEDINGS{goyal2017making,
  AUTHOR = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  BOOKTITLE = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  DATE = {2017},
  PAGES = {6904--6913},
  TITLE = {Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
}

@INPROCEEDINGS{hudson2019gqa,
  AUTHOR = {Hudson, Drew A and Manning, Christopher D},
  BOOKTITLE = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  DATE = {2019},
  PAGES = {6700--6709},
  TITLE = {Gqa: A new dataset for real-world visual reasoning and compositional question answering},
}

@ARTICLE{shekhar2017foil,
  AUTHOR = {Shekhar, Ravi and Pezzelle, Sandro and Klimovich, Yauhen and Herbelot, Aurélie and Nabi, Moin and Sangineto, Enver and Bernardi, Raffaella},
  DATE = {2017},
  JOURNALTITLE = {arXiv preprint arXiv:1705.01359},
  TITLE = {Foil it! find one mismatch between image and language caption},
}

@ARTICLE{ribeiro2020beyond,
  AUTHOR = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2005.04118},
  TITLE = {Beyond accuracy: Behavioral testing of NLP models with CheckList},
}

@INPROCEEDINGS{parcalabescu-etal-2022-valse,
  AUTHOR = {Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/2022.acl-long.567},
  BOOKTITLE = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  DATE = {2022-05},
  PAGES = {8253--8280},
  TITLE = {{VALSE}: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena},
}

@ARTICLE{sheng2019woman,
  AUTHOR = {Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1909.01326},
  TITLE = {The woman worked as a babysitter: On biases in language generation},
}

@INPROCEEDINGS{dhamala2021bold,
  AUTHOR = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  BOOKTITLE = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  DATE = {2021},
  PAGES = {862--872},
  TITLE = {Bold: Dataset and metrics for measuring biases in open-ended language generation},
}

@ARTICLE{prabhu2020large,
  AUTHOR = {Prabhu, Vinay Uday and Birhane, Abeba},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2006.16923},
  TITLE = {Large image datasets: A pyrrhic win for computer vision?},
}

@ARTICLE{birhane2021multimodal,
  AUTHOR = {Birhane, Abeba and Prabhu, Vinay Uday and Kahembwe, Emmanuel},
  DATE = {2021},
  JOURNALTITLE = {arXiv preprint arXiv:2110.01963},
  TITLE = {Multimodal datasets: misogyny, pornography, and malignant stereotypes},
}

@ARTICLE{strubell2019energy,
  AUTHOR = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1906.02243},
  TITLE = {Energy and policy considerations for deep learning in NLP},
}

@ARTICLE{lottick2019energy,
  AUTHOR = {Lottick, Kadan and Susai, Silvia and Friedler, Sorelle A and Wilson, Jonathan P},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1911.08354},
  TITLE = {Energy Usage Reports: Environmental awareness as part of algorithmic accountability},
}

@ARTICLE{henderson2020towards,
  AUTHOR = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  DATE = {2020},
  JOURNALTITLE = {Journal of Machine Learning Research},
  NUMBER = {248},
  PAGES = {1--43},
  TITLE = {Towards the systematic reporting of the energy and carbon footprints of machine learning},
  VOLUME = {21},
}

@INPROCEEDINGS{guo2016ms,
  AUTHOR = {Guo, Yandong and Zhang, Lei and Hu, Yuxiao and He, Xiaodong and Gao, Jianfeng},
  ORGANIZATION = {Springer},
  BOOKTITLE = {European conference on computer vision},
  DATE = {2016},
  PAGES = {87--102},
  TITLE = {Ms-celeb-1m: A dataset and benchmark for large-scale face recognition},
}

@INPROCEEDINGS{sun,
  AUTHOR = {Xiao, Jianxiong and Hays, James and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},
  BOOKTITLE = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  DATE = {2010},
  DOI = {10.1109/CVPR.2010.5539970},
  PAGES = {3485--3492},
  TITLE = {SUN database: Large-scale scene recognition from abbey to zoo},
}

@ARTICLE{pascalvoc,
  ABSTRACT = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
  AUTHOR = {Everingham, Mark and {van Gool}, Luc and Williams, {Christopher K. I.} and Winn, John and Zisserman, Andrew},
  LANGUAGE = {English},
  PUBLISHER = {Springer Netherlands},
  DATE = {2010-06},
  DOI = {10.1007/s11263-009-0275-4},
  ISSN = {0920-5691},
  JOURNALTITLE = {International Journal of Computer Vision},
  KEYWORDS = {Benchmark,Database,Object detection,Object recognition},
  NUMBER = {2},
  PAGES = {303--338},
  TITLE = {The PASCAL Visual Object Classes (VOC) Challenge},
  VOLUME = {88},
}

@ARTICLE{WordNet,
  AUTHOR = {Fellbaum, Christiane D.},
  DATE = {2000},
  JOURNALTITLE = {Language},
  PAGES = {706},
  TITLE = {WordNet : an electronic lexical database},
  VOLUME = {76},
}

@INPROCEEDINGS{Socher10connectingmodalities,
  AUTHOR = {Socher, Richard and Fei-fei, Li},
  BOOKTITLE = {In IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  DATE = {2010},
  TITLE = {Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora},
}

@ARTICLE{5487377,
  AUTHOR = {Yao, Benjamin Z. and Yang, Xiong and Lin, Liang and Lee, Mun Wai and Zhu, Song-Chun},
  DATE = {2010},
  DOI = {10.1109/JPROC.2010.2050411},
  JOURNALTITLE = {Proceedings of the IEEE},
  NUMBER = {8},
  PAGES = {1485--1508},
  TITLE = {I2T: Image Parsing to Text Description},
  VOLUME = {98},
}

@INPROCEEDINGS{vinyals,
  AUTHOR = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  DATE = {2015-06},
  DOI = {10.1109/CVPR.2015.7298935},
  PAGES = {3156--3164},
  TITLE = {Show and tell: A neural image caption generator},
}

@MISC{karpthy1,
  AUTHOR = {Karpathy, Andrej and Fei-Fei, Li},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1412.2306},
  DATE = {2014},
  DOI = {10.48550/ARXIV.1412.2306},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
}

@MISC{xu1,
  AUTHOR = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1502.03044},
  DATE = {2015},
  DOI = {10.48550/ARXIV.1502.03044},
  KEYWORDS = {Machine Learning (cs.LG),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
}

@MISC{yao1,
  AUTHOR = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1809.07041},
  DATE = {2018},
  DOI = {10.48550/ARXIV.1809.07041},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Exploring Visual Relationship for Image Captioning},
}

@INPROCEEDINGS{devlin-etal-2019-bert,
  AUTHOR = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  LOCATION = {Minneapolis, Minnesota},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/N19-1423},
  DATE = {2019-06},
  DOI = {10.18653/v1/N19-1423},
  PAGES = {4171--4186},
  TITLE = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
}

@INPROCEEDINGS{HerdadeKBS19,
  AUTHOR = {Herdade, Simao and Kappeler, Armin and Boakye, Kofi and Soares, Joao},
  URL = {http://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words},
  DATE = {2019},
  PAGES = {11135--11145},
  TITLE = {Image Captioning: Transforming Objects into Words},
}

@INPROCEEDINGS{huang1,
  AUTHOR = {Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  DATE = {2019-10},
  DOI = {10.1109/ICCV.2019.00473},
  PAGES = {4633--4642},
  TITLE = {Attention on Attention for Image Captioning},
}

@INPROCEEDINGS{NIPS2017_3f5ee243,
  AUTHOR = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  EDITOR = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2017},
  TITLE = {Attention is All you Need},
  VOLUME = {30},
}

@INPROCEEDINGS{spice,
  ABSTRACT = {There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?},
  AUTHOR = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  EDITOR = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {Computer Vision -- ECCV 2016},
  DATE = {2016},
  ISBN = {978-3-319-46454-1},
  PAGES = {382--398},
  TITLE = {SPICE: Semantic Propositional Image Caption Evaluation},
}

@INPROCEEDINGS{meteor,
  AUTHOR = {Banerjee, Satanjeev and Lavie, Alon},
  LOCATION = {Ann Arbor, Michigan},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/W05-0909},
  BOOKTITLE = {Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
  DATE = {2005-06},
  PAGES = {65--72},
  TITLE = {{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments},
}

@INPROCEEDINGS{lin-2004-rouge,
  AUTHOR = {Lin, Chin-Yew},
  LOCATION = {Barcelona, Spain},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/W04-1013},
  BOOKTITLE = {Text Summarization Branches Out},
  DATE = {2004-07},
  PAGES = {74--81},
  TITLE = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
}

@INPROCEEDINGS{cider,
  AUTHOR = {Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi},
  BOOKTITLE = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  DATE = {2015},
  DOI = {10.1109/CVPR.2015.7299087},
  PAGES = {4566--4575},
  TITLE = {CIDEr: Consensus-based image description evaluation},
}

@INPROCEEDINGS{papineni-etal-2002-bleu,
  AUTHOR = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  LOCATION = {Philadelphia, Pennsylvania, USA},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/P02-1040},
  BOOKTITLE = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  DATE = {2002-07},
  DOI = {10.3115/1073083.1073135},
  PAGES = {311--318},
  TITLE = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
}

@INPROCEEDINGS{8578734,
  AUTHOR = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  BOOKTITLE = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  DATE = {2018},
  DOI = {10.1109/CVPR.2018.00636},
  PAGES = {6077--6086},
  TITLE = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
}

@MISC{rfnet,
  AUTHOR = {Jiang, Wenhao and Ma, Lin and Jiang, Yu-Gang and Liu, Wei and Zhang, Tong},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1807.09986},
  DATE = {2018},
  DOI = {10.48550/ARXIV.1807.09986},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Recurrent Fusion Network for Image Captioning},
}

@INPROCEEDINGS{8099614,
  AUTHOR = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  BOOKTITLE = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  DATE = {2017},
  DOI = {10.1109/CVPR.2017.131},
  PAGES = {1179--1195},
  TITLE = {Self-Critical Sequence Training for Image Captioning},
}

@INPROCEEDINGS{Yang_2019_CVPR,
  AUTHOR = {Yang, Xu and Tang, Kaihua and Zhang, Hanwang and Cai, Jianfei},
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  DATE = {2019-06},
  TITLE = {Auto-Encoding Scene Graphs for Image Captioning},
}

@MISC{GCN-LSTM,
  AUTHOR = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1809.07041},
  DATE = {2018},
  DOI = {10.48550/ARXIV.1809.07041},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Exploring Visual Relationship for Image Captioning},
}

@ARTICLE{galanter2016generative,
  AUTHOR = {Galanter, Philip},
  PUBLISHER = {John Wiley \& Sons Hoboken, NJ},
  DATE = {2016},
  JOURNALTITLE = {A Companion to Digital Art},
  PAGES = {631},
  TITLE = {Generative art theory},
  VOLUME = {1},
}

@MISC{mordvintsev_2015,
  AUTHOR = {Mordvintsev, Alexander},
  PUBLISHER = {Google},
  URL = {https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
  DATE = {2015-06},
  JOURNALTITLE = {Google AI Blog},
  TITLE = {Inceptionism: Going Deeper into Neural Networks},
}

@MISC{tensorflow2015,
  PUBLISHER = {tensorflow.org},
  URL = {https://www.tensorflow.org/tutorials/generative/deepdream},
  DATE = {2015},
  NOTE = {Google Colab available from tensorflow.org},
  TITLE = {DeepDream},
}

@MISC{StyleTransfer,
  AUTHOR = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1508.06576},
  DATE = {2016},
  DOI = {10.48550/ARXIV.1508.06576},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Neural and Evolutionary Computing (cs.NE),Neurons and Cognition (q-bio.NC),FOS: Computer and information sciences,FOS: Computer and information sciences,FOS: Biological sciences,FOS: Biological sciences},
  TITLE = {A Neural Algorithm of Artistic Style},
}

@INPROCEEDINGS{NIPS2014_5ca3e9b1,
  AUTHOR = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  EDITOR = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2014},
  TITLE = {Generative Adversarial Nets},
  VOLUME = {27},
}

@INPROCEEDINGS{karras2019style,
  AUTHOR = {Karras, Tero and Laine, Samuli and Aila, Timo},
  BOOKTITLE = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  DATE = {2019},
  PAGES = {4401--4410},
  TITLE = {A style-based generator architecture for generative adversarial networks},
}

@MISC{morris_2022,
  AUTHOR = {Morris, Jack},
  URL = {https://www.bibme.org/bibtex/website-citation},
  DATE = {2022-01},
  TITLE = {The Weird and Wonderful World of AI Art},
}

@INPROCEEDINGS{8477754,
  AUTHOR = {Soderlund, Jacob and Blair, Alan},
  BOOKTITLE = {2018 IEEE Congress on Evolutionary Computation (CEC)},
  DATE = {2018},
  DOI = {10.1109/CEC.2018.8477754},
  PAGES = {1--8},
  TITLE = {Adversarial Image Generation Using Evolution and Deep Learning},
}

@ARTICLE{StyleGAN,
  AUTHOR = {Patashnik, Or and Wu, Zongze and Shechtman, Eli and Cohen{-}Or, Daniel and Lischinski, Dani},
  URL = {https://arxiv.org/abs/2103.17249},
  DATE = {2021},
  EPRINT = {2103.17249},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery},
}

@ARTICLE{DiffusionModels,
  AUTHOR = {Dhariwal, Prafulla and Nichol, Alex},
  URL = {https://arxiv.org/abs/2105.05233},
  DATE = {2021},
  EPRINT = {2105.05233},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {Diffusion Models Beat GANs on Image Synthesis},
}

@MISC{ruDALLE,
  AUTHOR = {Shonenkov, Alex},
  URL = {https://github.com/ai-forever/ru-dalle},
  DATE = {2021},
  TITLE = {ruDALL-E},
}

@MISC{DALLEmini,
  AUTHOR = {Boris, Dayma},
  URL = {https://huggingface.co/spaces/dalle-mini/dalle-mini},
  DATE = {2022},
  TITLE = {DALL·E mini},
}

@MISC{DALLEpytorch,
  AUTHOR = {OpenAI},
  URL = {https://github.com/openai/DALL-E},
  DATE = {2021},
  TITLE = {DALL-E},
}

@INPROCEEDINGS{liu2022design,
  AUTHOR = {Liu, Vivian and Chilton, Lydia B},
  BOOKTITLE = {CHI Conference on Human Factors in Computing Systems},
  DATE = {2022},
  PAGES = {1--23},
  TITLE = {Design Guidelines for Prompt Engineering Text-to-Image Generative Models},
}

@ARTICLE{LAION,
  AUTHOR = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  URL = {https://arxiv.org/abs/2111.02114},
  DATE = {2021},
  EPRINT = {2111.02114},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {{LAION-400M:} Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},
}

@MISC{WZRD,
  AUTHOR = {WZRD},
  URL = {https://wzrd.ai/},
  DATE = {2020},
  TITLE = {WZRD},
}

@MISC{DALLE2,
  AUTHOR = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2204.06125},
  DATE = {2022},
  DOI = {10.48550/ARXIV.2204.06125},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
}

@MISC{bias,
  AUTHOR = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2012.02516},
  DATE = {2020},
  DOI = {10.48550/ARXIV.2012.02516},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {A Note on Data Biases in Generative Models},
}

@INPROCEEDINGS{Radford2019LanguageMA,
  AUTHOR = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  DATE = {2019},
  TITLE = {Language Models are Unsupervised Multitask Learners},
}

@MISC{GPT3,
  AUTHOR = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2005.14165},
  DATE = {2020},
  DOI = {10.48550/ARXIV.2005.14165},
  KEYWORDS = {Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Language Models are Few-Shot Learners},
}

@MISC{environment,
  AUTHOR = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1906.02243},
  DATE = {2019},
  DOI = {10.48550/ARXIV.1906.02243},
  KEYWORDS = {Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Energy and Policy Considerations for Deep Learning in NLP},
}

@MISC{BERT,
  AUTHOR = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1810.04805},
  DATE = {2018},
  DOI = {10.48550/ARXIV.1810.04805},
  KEYWORDS = {Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
}

@INPROCEEDINGS{attention,
  AUTHOR = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  EDITOR = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2017},
  TITLE = {Attention is All you Need},
  VOLUME = {30},
}

@ARTICLE{3D,
  AUTHOR = {Mccormack, Jon and Gambardella, Camilo Cruz},
  DATE = {2022},
  DOI = {10.1109/TEVC.2021.3095156},
  JOURNALTITLE = {IEEE Transactions on Evolutionary Computation},
  NUMBER = {1},
  PAGES = {88--99},
  TITLE = {Growing and Evolving 3-D Prints},
  VOLUME = {26},
}

@ARTICLE{misconduct,
  AUTHOR = {Dehouche, Nassim},
  DATE = {2021},
  JOURNALTITLE = {Ethics in Science and Environmental Politics},
  PAGES = {17--23},
  TITLE = {Plagiarism in the age of massive Generative Pre-trained Transformers (GPT-3)},
  VOLUME = {21},
}

@INPROCEEDINGS{bias_ML,
  AUTHOR = {Srinivasan, Ramya and Uchino, Kanji},
  BOOKTITLE = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  DATE = {2021},
  PAGES = {41--51},
  TITLE = {Biases in generative art: A causal look from the lens of art history},
}

@INPROCEEDINGS{qiao2022initial,
  AUTHOR = {Qiao, Han and Liu, Vivian and Chilton, Lydia},
  BOOKTITLE = {Creativity and Cognition},
  DATE = {2022},
  PAGES = {15--28},
  TITLE = {Initial Images: Using Image Prompts to Improve Subject Representation in Multimodal AI Generated Art},
}

@MISC{unrealEngine,
  AUTHOR = {Aran, Komatsuzaki},
  PUBLISHER = {Twitter},
  URL = {https://twitter.com/arankomatsuzaki/status/1399471244760649729},
  DATE = {2021},
  TITLE = {When you generate images with VQGAN CLIP, the image quality dramatically improves if you add "unreal engine" to your prompt. People are now calling this "unreal engine trick"},
}

@MISC{NFT,
  AUTHOR = {Wang, Qin and Li, Rujia and Wang, Qi and Chen, Shiping},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2105.07447},
  DATE = {2021},
  DOI = {10.48550/ARXIV.2105.07447},
  KEYWORDS = {Cryptography and Security (cs.CR),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Non-Fungible Token (NFT): Overview, Evaluation, Opportunities and Challenges},
}

@ARTICLE{EfficientNet,
  AUTHOR = {Tan, Mingxing and Le, Quoc V.},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1905.11946},
  DATE = {2019},
  DOI = {10.48550/ARXIV.1905.11946},
  KEYWORDS = {Machine Learning (cs.LG),Computer Vision and Pattern Recognition (cs.CV),Machine Learning (stat.ML),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
}

@MISC{SimCLR,
  AUTHOR = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2002.05709},
  DATE = {2020},
  DOI = {10.48550/ARXIV.2002.05709},
  KEYWORDS = {Machine Learning (cs.LG),Computer Vision and Pattern Recognition (cs.CV),Machine Learning (stat.ML),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {A Simple Framework for Contrastive Learning of Visual Representations},
}

@MISC{BYOL,
  AUTHOR = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2006.07733},
  DATE = {2020},
  DOI = {10.48550/ARXIV.2006.07733},
  KEYWORDS = {Machine Learning (cs.LG),Computer Vision and Pattern Recognition (cs.CV),Machine Learning (stat.ML),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Bootstrap your own latent: A new approach to self-supervised Learning},
}

@MISC{COCO,
  AUTHOR = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1405.0312},
  DATE = {2014},
  DOI = {10.48550/ARXIV.1405.0312},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Microsoft COCO: Common Objects in Context},
}

@MISC{meshed_memory,
  AUTHOR = {Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1912.08226},
  DATE = {2019},
  DOI = {10.48550/ARXIV.1912.08226},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Meshed-Memory Transformer for Image Captioning},
}

@ARTICLE{VAE,
  AUTHOR = {Kingma, Diederik P. and Welling, Max},
  URL = {http://arxiv.org/abs/1906.02691},
  DATE = {2019},
  EPRINT = {1906.02691},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {An Introduction to Variational Autoencoders},
}

@MISC{DALLE,
  AUTHOR = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2102.12092},
  DATE = {2021},
  DOI = {10.48550/ARXIV.2102.12092},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Zero-Shot Text-to-Image Generation},
}

@MISC{CLIP,
  AUTHOR = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2103.00020},
  DATE = {2021},
  DOI = {10.48550/ARXIV.2103.00020},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Learning Transferable Visual Models From Natural Language Supervision},
}

@MISC{VilBert,
  AUTHOR = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1908.02265},
  DATE = {2019},
  DOI = {10.48550/ARXIV.1908.02265},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
}

@MISC{Flamingo,
  AUTHOR = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2204.14198},
  DATE = {2022},
  DOI = {10.48550/ARXIV.2204.14198},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Artificial Intelligence (cs.AI),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Flamingo: a Visual Language Model for Few-Shot Learning},
}

@MISC{GAN,
  AUTHOR = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/1406.2661},
  DATE = {2014},
  DOI = {10.48550/ARXIV.1406.2661},
  KEYWORDS = {Machine Learning (stat.ML),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Generative Adversarial Networks},
}

@MISC{GLIDE,
  AUTHOR = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2112.10741},
  DATE = {2021},
  DOI = {10.48550/ARXIV.2112.10741},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV),Graphics (cs.GR),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
}

@MISC{Pathways,
  AUTHOR = {Barham, Paul and Chowdhery, Aakanksha and Dean, Jeff and Ghemawat, Sanjay and Hand, Steven and Hurt, Dan and Isard, Michael and Lim, Hyeontaek and Pang, Ruoming and Roy, Sudip and Saeta, Brennan and Schuh, Parker and Sepassi, Ryan and Shafey, Laurent El and Thekkath, Chandramohan A. and Wu, Yonghui},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/abs/2203.12533},
  DATE = {2022},
  DOI = {10.48550/ARXIV.2203.12533},
  KEYWORDS = {Distributed,Parallel,and Cluster Computing (cs.DC),Machine Learning (cs.LG),FOS: Computer and information sciences,FOS: Computer and information sciences},
  TITLE = {Pathways: Asynchronous Distributed Dataflow for ML},
}

@ARTICLE{explainaility,
  AUTHOR = {Joshi, Gargi and Walambe, Rahee and Kotecha, Ketan},
  DATE = {2021},
  DOI = {10.1109/ACCESS.2021.3070212},
  JOURNALTITLE = {IEEE Access},
  PAGES = {59800--59821},
  TITLE = {A Review on Explainability in Multimodal Deep Neural Nets},
  VOLUME = {9},
}

@ARTICLE{ALIGN,
  AUTHOR = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi{-}Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yun{-}Hsuan and Li, Zhen and Duerig, Tom},
  URL = {https://arxiv.org/abs/2102.05918},
  DATE = {2021},
  EPRINT = {2102.05918},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
}

@ARTICLE{yuan2021florence,
  AUTHOR = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
  DATE = {2021},
  JOURNALTITLE = {arXiv preprint arXiv:2111.11432},
  TITLE = {Florence: A New Foundation Model for Computer Vision},
}

@ARTICLE{agirre2009study,
  AUTHOR = {Agirre, Eneko and Alfonseca, Enrique and Hall, Keith and Kravalova, Jana and Pasca, Marius and Soroa, Aitor},
  DATE = {2009},
  TITLE = {A study on similarity and relatedness using distributional and wordnet-based approaches},
}

@ARTICLE{ailem2018probabilistic,
  AUTHOR = {Ailem, Melissa and Zhang, Bowen and Bellet, Aurelien and Denis, Pascal and Sha, Fei},
  DATE = {2018},
  TITLE = {A probabilistic model for joint learning of word embeddings from texts and images},
}

@INPROCEEDINGS{bosch2007image,
  AUTHOR = {Bosch, Anna and Zisserman, Andrew and Munoz, Xavier},
  ORGANIZATION = {Ieee},
  BOOKTITLE = {2007 IEEE 11th international conference on computer vision},
  DATE = {2007},
  PAGES = {1--8},
  TITLE = {Image classification using random forests and ferns},
}

@ARTICLE{bruni2014multimodal,
  AUTHOR = {Bruni, Elia and Tran, Nam-Khanh and Baroni, Marco},
  DATE = {2014},
  JOURNALTITLE = {Journal of artificial intelligence research},
  PAGES = {1--47},
  TITLE = {Multimodal distributional semantics},
  VOLUME = {49},
}

@ARTICLE{brysbaert2014concreteness,
  AUTHOR = {Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  PUBLISHER = {Springer},
  DATE = {2014},
  JOURNALTITLE = {Behavior research methods},
  NUMBER = {3},
  PAGES = {904--911},
  TITLE = {Concreteness ratings for 40 thousand generally known English word lemmas},
  VOLUME = {46},
}

@INPROCEEDINGS{collell2017imagined,
  AUTHOR = {Collell, Guillem and Zhang, Ted and Moens, Marie-Francine},
  BOOKTITLE = {Proceedings of the AAAI Conference on Artificial Intelligence},
  DATE = {2017},
  NUMBER = {1},
  TITLE = {Imagined visual representations as multimodal embeddings},
  VOLUME = {31},
}

@ARTICLE{devlin2018bert,
  AUTHOR = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  DATE = {2018},
  JOURNALTITLE = {arXiv preprint arXiv:1810.04805},
  TITLE = {Bert: Pre-training of deep bidirectional transformers for language understanding},
}

@ARTICLE{devereux2014centre,
  AUTHOR = {Devereux, Barry J and Tyler, Lorraine K and Geertzen, Jeroen and Randall, Billi},
  PUBLISHER = {Springer},
  DATE = {2014},
  JOURNALTITLE = {Behavior research methods},
  NUMBER = {4},
  PAGES = {1119--1127},
  TITLE = {The Centre for Speech, Language and the Brain (CSLB) concept property norms},
  VOLUME = {46},
}

@INPROCEEDINGS{esser2021taming,
  AUTHOR = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  BOOKTITLE = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  DATE = {2021},
  PAGES = {12873--12883},
  TITLE = {Taming transformers for high-resolution image synthesis},
}

@ARTICLE{harnad1990symbol,
  AUTHOR = {Harnad, Stevan},
  PUBLISHER = {Elsevier},
  DATE = {1990},
  JOURNALTITLE = {Physica D: Nonlinear Phenomena},
  NUMBER = {1-3},
  PAGES = {335--346},
  TITLE = {The symbol grounding problem},
  VOLUME = {42},
}

@ARTICLE{harris1954distributional,
  AUTHOR = {Harris, Z and others},
  DATE = {1954},
  JOURNALTITLE = {Word World},
  NUMBER = {23},
  PAGES = {146--162},
  TITLE = {Distributional hypothesis},
  VOLUME = {10},
}

@INPROCEEDINGS{hill2014learning,
  AUTHOR = {Hill, Felix and Korhonen, Anna},
  BOOKTITLE = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  DATE = {2014},
  PAGES = {255--265},
  TITLE = {Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean},
}

@ARTICLE{hill2015simlex,
  AUTHOR = {Hill, Felix and Reichart, Roi and Korhonen, Anna},
  PUBLISHER = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  DATE = {2015},
  JOURNALTITLE = {Computational Linguistics},
  NUMBER = {4},
  PAGES = {665--695},
  TITLE = {Simlex-999: Evaluating semantic models with (genuine) similarity estimation},
  VOLUME = {41},
}

@ARTICLE{hochreiter1997long,
  AUTHOR = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  PUBLISHER = {MIT Press},
  DATE = {1997},
  JOURNALTITLE = {Neural computation},
  NUMBER = {8},
  PAGES = {1735--1780},
  TITLE = {Long short-term memory},
  VOLUME = {9},
}

@INPROCEEDINGS{hu2021unit,
  AUTHOR = {Hu, Ronghang and Singh, Amanpreet},
  BOOKTITLE = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  DATE = {2021},
  PAGES = {1439--1449},
  TITLE = {Unit: Multimodal multitask learning with a unified transformer},
}

@ARTICLE{ive2019distilling,
  AUTHOR = {Ive, Julia and Madhyastha, Pranava and Specia, Lucia},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1906.07701},
  TITLE = {Distilling translations with visual awareness},
}

@INPROCEEDINGS{kiela2014learning,
  AUTHOR = {Kiela, Douwe and Bottou, Léon},
  BOOKTITLE = {Proceedings of the 2014 Conference on empirical methods in natural language processing (EMNLP)},
  DATE = {2014},
  PAGES = {36--45},
  TITLE = {Learning image embeddings using convolutional neural networks for improved multi-modal semantics},
}

@ARTICLE{kiela2017learning,
  AUTHOR = {Kiela, Douwe and Conneau, Alexis and Jabri, Allan and Nickel, Maximilian},
  DATE = {2017},
  JOURNALTITLE = {arXiv preprint arXiv:1707.06320},
  TITLE = {Learning visually grounded sentence representations},
}

@INPROCEEDINGS{kiros2018illustrative,
  AUTHOR = {Kiros, Jamie and Chan, William and Hinton, Geoffrey},
  BOOKTITLE = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  DATE = {2018},
  PAGES = {922--933},
  TITLE = {Illustrative language understanding: Large-scale visual grounding with image search},
}

@INPROCEEDINGS{kottur2016visual,
  AUTHOR = {Kottur, Satwik and Vedantam, Ramakrishna and Moura, José MF and Parikh, Devi},
  BOOKTITLE = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  DATE = {2016},
  PAGES = {4985--4994},
  TITLE = {Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes},
}

@ARTICLE{krizhevsky2012imagenet,
  AUTHOR = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  DATE = {2012},
  JOURNALTITLE = {Advances in neural information processing systems},
  TITLE = {Imagenet classification with deep convolutional neural networks},
  VOLUME = {25},
}

@ARTICLE{lazaridou2015combining,
  AUTHOR = {Lazaridou, Angeliki and Pham, Nghia The and Baroni, Marco},
  DATE = {2015},
  JOURNALTITLE = {arXiv preprint arXiv:1501.02598},
  TITLE = {Combining language and vision with a multimodal skip-gram model},
}

@INPROCEEDINGS{lin2014microsoft,
  AUTHOR = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C Lawrence},
  ORGANIZATION = {Springer},
  BOOKTITLE = {European conference on computer vision},
  DATE = {2014},
  PAGES = {740--755},
  TITLE = {Microsoft coco: Common objects in context},
}

@ARTICLE{lu2022imagination,
  AUTHOR = {Lu, Yujie and Zhu, Wanrong and Wang, Xin Eric and Eckstein, Miguel and Wang, William Yang},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2204.08535},
  TITLE = {Imagination-Augmented Natural Language Understanding},
}

@ARTICLE{mikolov2013efficient,
  AUTHOR = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  DATE = {2013},
  JOURNALTITLE = {arXiv preprint arXiv:1301.3781},
  TITLE = {Efficient estimation of word representations in vector space},
}

@INPROCEEDINGS{pennington2014glove,
  AUTHOR = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  BOOKTITLE = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  DATE = {2014},
  PAGES = {1532--1543},
  TITLE = {Glove: Global vectors for word representation},
}

@ARTICLE{pezzelle2021word,
  AUTHOR = {Pezzelle, Sandro and Takmaz, Ece and Fernández, Raquel},
  PUBLISHER = {MIT Press},
  DATE = {2021},
  JOURNALTITLE = {Transactions of the Association for Computational Linguistics},
  PAGES = {1563--1579},
  TITLE = {Word representation learning in multimodal pre-trained transformers: An intrinsic evaluation},
  VOLUME = {9},
}

@ARTICLE{russakovsky2015imagenet,
  AUTHOR = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  PUBLISHER = {Springer},
  DATE = {2015},
  JOURNALTITLE = {International journal of computer vision},
  NUMBER = {3},
  PAGES = {211--252},
  TITLE = {Imagenet large scale visual recognition challenge},
  VOLUME = {115},
}

@INPROCEEDINGS{singh2022flava,
  AUTHOR = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  DATE = {2022},
  PAGES = {15638--15650},
  TITLE = {Flava: A foundational language and vision alignment model},
}

@INPROCEEDINGS{silberer2014learning,
  AUTHOR = {Silberer, Carina and Lapata, Mirella},
  BOOKTITLE = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  DATE = {2014},
  PAGES = {721--732},
  TITLE = {Learning grounded meaning representations with autoencoders},
}

@ARTICLE{sun2021multimodal,
  AUTHOR = {Sun, Qingfeng and Wang, Yujing and Xu, Can and Zheng, Kai and Yang, Yaming and Hu, Huang and Xu, Fei and Zhang, Jessica and Geng, Xiubo and Jiang, Daxin},
  DATE = {2021},
  JOURNALTITLE = {arXiv preprint arXiv:2110.08515},
  TITLE = {Multimodal dialogue response generation},
}

@ARTICLE{tan2020vokenization,
  AUTHOR = {Tan, Hao and Bansal, Mohit},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2010.06775},
  TITLE = {Vokenization: Improving language understanding with contextualized, visual-grounded supervision},
}

@ARTICLE{wang2018glue,
  AUTHOR = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  DATE = {2018},
  JOURNALTITLE = {arXiv preprint arXiv:1804.07461},
  TITLE = {GLUE: A multi-task benchmark and analysis platform for natural language understanding},
}

@ARTICLE{zellers2018swag,
  AUTHOR = {Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
  DATE = {2018},
  JOURNALTITLE = {arXiv preprint arXiv:1808.05326},
  TITLE = {Swag: A large-scale adversarial dataset for grounded commonsense inference},
}

@MANUAL{rlang,
  AUTHOR = {{R Core Team}},
  LOCATION = {Vienna, Austria},
  ORGANIZATION = {R Foundation for Statistical Computing},
  URL = {https://www.R-project.org/},
  DATE = {2018},
  TITLE = {R: A Language and Environment for Statistical Computing},
}

@INPROCEEDINGS{radford2021learning,
  AUTHOR = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  ORGANIZATION = {PMLR},
  BOOKTITLE = {International Conference on Machine Learning},
  DATE = {2021},
  PAGES = {8748--8763},
  TITLE = {Learning transferable visual models from natural language supervision},
}

@ARTICLE{baevski2022data2vec,
  AUTHOR = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2202.03555},
  TITLE = {Data2vec: A general framework for self-supervised learning in speech, vision and language},
}

@ARTICLE{bao2021beit,
  AUTHOR = {Bao, Hangbo and Dong, Li and Wei, Furu},
  DATE = {2021},
  JOURNALTITLE = {arXiv preprint arXiv:2106.08254},
  TITLE = {Beit: Bert pre-training of image transformers},
}

@ARTICLE{radford2019language,
  AUTHOR = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  DATE = {2019},
  JOURNALTITLE = {OpenAI blog},
  NUMBER = {8},
  PAGES = {9},
  TITLE = {Language models are unsupervised multitask learners},
  VOLUME = {1},
}

@ARTICLE{shen2021much,
  AUTHOR = {Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
  DATE = {2021},
  JOURNALTITLE = {arXiv preprint arXiv:2107.06383},
  TITLE = {How Much Can CLIP Benefit Vision-and-Language Tasks?},
}

@INPROCEEDINGS{silberer2012grounded,
  AUTHOR = {Silberer, Carina and Lapata, Mirella},
  ORGANIZATION = {ACL (Association for Computational Linguistics)},
  BOOKTITLE = {Tsujii J, Henderson J, Paşca M, editors. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning; 2012 Jul 12--14; Jeju Island, Korea. Stroudsburg: ACL; 2012. p. 1423-33.},
  DATE = {2012},
  TITLE = {Grounded models of semantic representation},
}

@ARTICLE{sennrich2015neural,
  AUTHOR = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  DATE = {2015},
  JOURNALTITLE = {arXiv preprint arXiv:1508.07909},
  TITLE = {Neural machine translation of rare words with subword units},
}

@ARTICLE{baevski2020wav2vec,
  AUTHOR = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  DATE = {2020},
  JOURNALTITLE = {Advances in Neural Information Processing Systems},
  PAGES = {12449--12460},
  TITLE = {wav2vec 2.0: A framework for self-supervised learning of speech representations},
  VOLUME = {33},
}

@INPROCEEDINGS{chen2021empirical,
  AUTHOR = {Chen, Xinlei and Xie, Saining and He, Kaiming},
  BOOKTITLE = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  DATE = {2021},
  PAGES = {9640--9649},
  TITLE = {An empirical study of training self-supervised vision transformers},
}

@INPROCEEDINGS{he2022masked,
  AUTHOR = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  DATE = {2022},
  PAGES = {16000--16009},
  TITLE = {Masked autoencoders are scalable vision learners},
}

@ARTICLE{zhang2020contrastive,
  AUTHOR = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D and Langlotz, Curtis P},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2010.00747},
  TITLE = {Contrastive learning of medical visual representations from paired images and text},
}

@ONLINE{sutton2019bitterlesson,
  AUTHOR = {Sutton, R. S.},
  URL = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  DATE = {2019-03-13},
  TITLE = {The Bitter Lesson},
}

@ONLINE{openai2021clipblog,
  AUTHOR = {OpenAI},
  URL = {https://openai.com/blog/clip/},
  DATE = {2021-01-05},
  TITLE = {CLIP: Connection Text and Images},
}

@ONLINE{alford2021alignparams,
  AUTHOR = {Alford, A.},
  URL = {https://www.infoq.com/news/2021/07/google-vision-language-ai/},
  DATE = {2021-07-20},
  TITLE = {Google Announces 800M Parameter Vision-Language AI Model ALIGN},
}

@ONLINE{schuhmann2022laion,
  AUTHOR = {Schuhmann, C.},
  URL = {https://laion.ai/blog/laion-400-open-dataset/},
  DATE = {2022-07-07},
  TITLE = {Laion-400-Million Open Dataset},
}

@ONLINE{solawetz2021florenceopen,
  AUTHOR = {Solawetz, J.},
  URL = {https://blog.roboflow.com/florence-a-new-foundational-model-for-computer-vision/},
  DATE = {2021-12-09},
  TITLE = {Florence: A New Foundation for Computer Vision},
}

@INPROCEEDINGS{wei2022masked,
  AUTHOR = {Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  DATE = {2022},
  PAGES = {14668--14678},
  TITLE = {Masked feature prediction for self-supervised visual pre-training},
}

@INPROCEEDINGS{jaegle2021perceiver,
  AUTHOR = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  ORGANIZATION = {PMLR},
  BOOKTITLE = {International conference on machine learning},
  DATE = {2021},
  PAGES = {4651--4664},
  TITLE = {Perceiver: General perception with iterative attention},
}

@ARTICLE{alayrac2022flamingo,
  AUTHOR = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2204.14198},
  TITLE = {Flamingo: a visual language model for few-shot learning},
}

@ARTICLE{lu2019vilbert,
  AUTHOR = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  DATE = {2019},
  JOURNALTITLE = {Advances in neural information processing systems},
  TITLE = {Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  VOLUME = {32},
}

@ARTICLE{uppal2022multimodal,
  AUTHOR = {Uppal, Shagun and Bhagat, Sarthak and Hazarika, Devamanyu and Majumder, Navonil and Poria, Soujanya and Zimmermann, Roger and Zadeh, Amir},
  PUBLISHER = {Elsevier},
  DATE = {2022},
  JOURNALTITLE = {Information Fusion},
  PAGES = {149--171},
  TITLE = {Multimodal research in vision and language: A review of current and emerging trends},
  VOLUME = {77},
}

@ARTICLE{hoffmann2022training,
  AUTHOR = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2203.15556},
  TITLE = {Training Compute-Optimal Large Language Models},
}

@INPROCEEDINGS{jia2021scaling,
  AUTHOR = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  ORGANIZATION = {PMLR},
  BOOKTITLE = {International Conference on Machine Learning},
  DATE = {2021},
  PAGES = {4904--4916},
  TITLE = {Scaling up visual and vision-language representation learning with noisy text supervision},
}

@ARTICLE{perez2021true,
  AUTHOR = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  DATE = {2021},
  JOURNALTITLE = {Advances in Neural Information Processing Systems},
  PAGES = {11054--11070},
  TITLE = {True few-shot learning with language models},
  VOLUME = {34},
}

@ARTICLE{zeng2022socratic,
  AUTHOR = {Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and others},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2204.00598},
  TITLE = {Socratic models: Composing zero-shot multimodal reasoning with language},
}

@ARTICLE{ren2015faster,
  AUTHOR = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  DATE = {2015},
  JOURNALTITLE = {Advances in neural information processing systems},
  TITLE = {Faster r-cnn: Towards real-time object detection with region proposal networks},
  VOLUME = {28},
}

@INPROCEEDINGS{krishnavisualgenome,
  AUTHOR = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and Bernstein, Michael and Fei-Fei, Li},
  URL = {https://arxiv.org/abs/1602.07332},
  DATE = {2016},
  TITLE = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
}

@ARTICLE{sikarwar2022efficacy,
  AUTHOR = {Sikarwar, Ankur and Kreiman, Gabriel},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2201.03965},
  TITLE = {On the efficacy of co-attention transformer layers in visual question answering},
}

@ARTICLE{chowdhery2022palm,
  AUTHOR = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  DATE = {2022},
  JOURNALTITLE = {arXiv preprint arXiv:2204.02311},
  TITLE = {Palm: Scaling language modeling with pathways},
}

@ARTICLE{das2017human,
  AUTHOR = {Das, Abhishek and Agrawal, Harsh and Zitnick, Larry and Parikh, Devi and Batra, Dhruv},
  PUBLISHER = {Elsevier},
  DATE = {2017},
  JOURNALTITLE = {Computer Vision and Image Understanding},
  PAGES = {90--100},
  TITLE = {Human attention in visual question answering: Do humans and deep networks look at the same regions?},
  VOLUME = {163},
}
